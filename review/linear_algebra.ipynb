{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4c3d773-c3e3-4756-82cc-3937ca187d5a",
   "metadata": {},
   "source": [
    "# Part 1: Linear Algebra"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c72965a-1c82-49ed-86dc-454f339c7b0a",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "In this section of the assignment, we will demonstrate the capabilities of Python to execute a number of linear-algebraic tasks, and describe which libraries we must use to replicate MATLAB's functionality as explained in the assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19d03b5-3ba0-4413-9fab-ca9e757fc792",
   "metadata": {},
   "source": [
    "## Import libraries\n",
    "To work with vectors, we will need to take advantage of Python's `numpy` library, which will allow us to define vectors and many of their operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9604e0ee-dd5c-4c21-a14e-1ddcabc9b51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f78a1e-1044-48f3-800b-0fe218e7d850",
   "metadata": {},
   "source": [
    "## Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9c8472-ac7c-4ae2-94a7-d95fd6b54f9d",
   "metadata": {},
   "source": [
    "### 1) Vectors\n",
    "Here, we are given a number of vectors to define, and then we are told to multiply them in a few combinations. Let's use `numpy` to define the given vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "258c0675-b91e-4dc8-9e10-4c2edfc70e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([0, 0, 1])\n",
    "b = np.array([1, 1, 0])\n",
    "c = np.array([0, -1, -1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34427dd-0038-4107-97a3-291feb422a83",
   "metadata": {},
   "source": [
    "Note that we are technically defining these as row vectors &mdash; unlike MATLAB, `numpy` has no inherent distinction between row and column vectors if said vectors are explicitly one-dimensional."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44e13ed-c749-430d-bea9-dfc7a8360c59",
   "metadata": {},
   "source": [
    "Now, let's calculate the scalar products that we're told to calculate. Here, we will use the `numpy.dot()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b1615ce-0226-4180-8a50-0f7949700116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, -1, -1]\n"
     ]
    }
   ],
   "source": [
    "products = [np.dot(a, b), np.dot(b, a), \n",
    "            np.dot(a, c), np.dot(b, c)]\n",
    "\n",
    "# These products are, by default, numpy's own int64 type, \n",
    "# so we're going to convert them to Python's built-in int type\n",
    "print([int(p) for p in products])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af53a50-d7b3-4fa8-b8a2-4e0decec90c3",
   "metadata": {},
   "source": [
    "Why do these results make sense geometrically? Well, let's take a closer look at the actual vectors involved in the multiplication operations.  \n",
    "\n",
    "$\\mathbf{a}\\cdot\\mathbf{b}=0$  \n",
    "Looking at the components of $\\mathbf{a}$ and $\\mathbf{b}$, the two vectors are *complementary*, meaning each non-zero component of one vector is in the same position as a zero component in the other vector. In linear algebra, this makes the two vectors *orthogonal*, or perpendicular, meaning their dot product is definitionally equal to $0$.  \n",
    "\n",
    "$\\mathbf{b}\\cdot\\mathbf{a}=0$  \n",
    "Another definitional aspect of the dot product is its *commutativity*; that is, the order of two dot-multiplied vectors does not matter. Thus, $\\mathbf{b}\\cdot\\mathbf{a}$ is equal to $\\mathbf{a}\\cdot\\mathbf{b}$, and both are $0$.  \n",
    "\n",
    "$\\mathbf{a}\\cdot\\mathbf{c}=-1$ and $\\mathbf{b}\\cdot\\mathbf{c}=-1$  \n",
    "These are both equal to $-1$, which makes sense as neither $\\mathbf{a}$ nor $\\mathbf{b}$ are orthogonal to $\\mathbf{c}$. In addition, since the sign is negative, this means that the angles between $\\mathbf{a}$ and $\\mathbf{c}$ as well as between $\\mathbf{b}$ and $\\mathbf{c}$ are greater than $90^{\\circ}$, or obtuse. The angles are also certainly less than $180^{\\circ}$, which would mean the vectors point in opposite directions, as if the angles were $180^{\\circ}$, the vectors' dot product would be equal to the negative product of the respective two vectors' magnitudes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4096521a-d597-4fbb-af5d-1c5afa4491f7",
   "metadata": {},
   "source": [
    "### 2) Matrices\n",
    "Now, we're moving onto matrices, which we can similarly use `numpy`'s array feature for:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2180eb71-ef71-4335-89c8-b6bc4af40b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[1, 0, 0],\n",
    "              [0, 1, 0],\n",
    "              [-1, 0, 1]])\n",
    "B = np.array([[1, 0, 0],\n",
    "              [0, 1, 0],\n",
    "              [1, 0, 1]])\n",
    "C = np.array([[0, -1, 0],\n",
    "              [1, 0, 0],\n",
    "              [0, 0, 0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729f4ae9-9f53-4686-84e0-53ea8c8ecdb6",
   "metadata": {},
   "source": [
    "We're now asked to multiply these matrices in a few given orders. Let's continue to use `numpy` to calculate these products, specifically the `numpy.matmul()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7db4dc9c-fd9e-4722-a6e3-1711ad292261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "[[ 0 -1  0]\n",
      " [ 1  0  0]\n",
      " [ 0 -1  0]]\n",
      "[[ 0 -1  0]\n",
      " [ 1  0  0]\n",
      " [ 0  0  0]]\n",
      "[[ 0  1  0]\n",
      " [-1  0  0]\n",
      " [ 0  1  0]]\n"
     ]
    }
   ],
   "source": [
    "# These three are pretty straightforward\n",
    "AB = np.matmul(A, B)\n",
    "BC = np.matmul(B, C)\n",
    "CB = np.matmul(C, B)\n",
    "\n",
    "# Now, one of our matrices is transposed\n",
    "# We can use numpy's transpose shorthand for this calculation\n",
    "BC_T = np.matmul(B, C.T)\n",
    "\n",
    "# Now, let's print our matrices\n",
    "print(str(AB) + \"\\n\" + str(BC) + \"\\n\"\n",
    "      + str(CB) + \"\\n\" + str(BC_T))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d706a90-3c76-4a8e-a993-eb59a8a8a537",
   "metadata": {},
   "source": [
    "Our results are as follows:  \n",
    "  \n",
    "$\\mathbf{AB}=\\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}\\;\\;$  \n",
    "$\\mathbf{BC}=\\begin{bmatrix} 0 & -1 & 0 \\\\ 1 & 0 & 0 \\\\ 0 & -1 & 0 \\end{bmatrix}\\;\\;$  \n",
    "$\\mathbf{CB}=\\begin{bmatrix} 0 & -1 & 0 \\\\ 1 & 0 & 0 \\\\ 0 & 0 & 0 \\end{bmatrix}\\;\\;$  \n",
    "$\\mathbf{BC^T}=\\begin{bmatrix} 0 & 1 & 0 \\\\ -1 & 0 & 0 \\\\ 0 & 1 & 0 \\end{bmatrix}\\;\\;$\n",
    "  \n",
    "We can also find the inverse of $\\mathbf{A}$ as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20f00e87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "A_inv = np.linalg.inv(A)\n",
    "print(A_inv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6244e759",
   "metadata": {},
   "source": [
    "Our result is as follows:  \n",
    "  \n",
    "$\\mathbf{A^{-1}}=\\begin{bmatrix}1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 1 & 0 & 1\\end{bmatrix}$\n",
    "\n",
    "Next, to explore the linear operation that is defined by $\\mathbf{C}$, let's multiply it by a few arbitrary vectors in $\\mathbb{R}^3$ and examine the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ccdf928-4ffa-41e5-9679-b5e122985dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([-1,  1,  0]), array([-5, 10,  0]), array([-3,  2,  0])]\n"
     ]
    }
   ],
   "source": [
    "# Define arbitrary vectors\n",
    "vect_1 = np.array([1, 1, 1])\n",
    "vect_2 = np.array([10, 5, 10])\n",
    "vect_3 = np.array([2, 3, 1])\n",
    "\n",
    "# Multiply each vector\n",
    "products = [np.matmul(C, vect_1), np.matmul(C, vect_2), np.matmul(C, vect_3)]\n",
    "print(products)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5704197-778d-4d62-a213-0c0abcb415b6",
   "metadata": {},
   "source": [
    "The results are as follows:  \n",
    "  \n",
    "$\\mathbf{Cv_{1}}=\\begin{bmatrix}-1 \\\\ 1 \\\\ 0\\end{bmatrix}\\;\\;$ \n",
    "$\\mathbf{Cv_{2}}=\\begin{bmatrix}-5 \\\\ 10 \\\\ 0\\end{bmatrix}\\;\\;$ \n",
    "$\\mathbf{Cv_{3}}=\\begin{bmatrix}-3 \\\\ 2 \\\\ 0\\end{bmatrix}$  \n",
    "  \n",
    "The resulting linear operation is pretty clear: the $z$-component of the vector is set to $0$, projecting the vector onto the $xy$-plane; the $x$- and $y$-components are then swapped, and the new $x$-component is negated, which both serve to rotate the projected vector $90^{\\circ}$ counterclockwise. The resultant vector will thus be an $xy$-projected version of the original vector rotated $90^{\\circ}$ from the original, and always orthogonal.  \n",
    "We know that $\\mathbf{C}$ is **not invertible**, as the matrix is neither full row- or column-rank (the last row and column are both the zero vector), and invertible matrices are always both full row-rank and column-rank."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abd4dd2-8fd4-40d8-9f20-a47ad8283a78",
   "metadata": {},
   "source": [
    "Now, we're asked which of $\\mathbf{a}$, $\\mathbf{b}$, or $\\mathbf{c}$ are eigenvectors of $\\mathbf{B}$. If a vector is an eigenvector of a matrix, the product of the matrix and vector will simply scale said vector. As such, let's test each of the three matrix products:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35983568-0537-4744-8116-0f003fffde7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1][0 0 1]\n",
      "\n",
      "[1 1 0][1 1 1]\n",
      "\n",
      "[ 0 -1 -1][ 0 -1 -1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "products = [np.matmul(B, a), np.matmul(B, b), np.matmul(B, c)]\n",
    "\n",
    "# Print products next to original vectors\n",
    "for [original, product] in zip([a, b, c], products):\n",
    "    print(str(original) + str(product) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e468e61-4a57-4a59-8c80-fdad566a4102",
   "metadata": {},
   "source": [
    "The results are as follows:  \n",
    "  \n",
    "$\\mathbf{Ba} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\end{bmatrix}\\;\\;$\n",
    "$\\mathbf{Bb} = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix}\\;\\;$\n",
    "$\\mathbf{Bc} = \\begin{bmatrix} 0 \\\\ -1 \\\\ -1 \\end{bmatrix}\\;\\;$  \n",
    "  \n",
    "We can see that $\\mathbf{Ba}$ and $\\mathbf{Bc}$ not only scaled the original two vectors, but actually preserved them entirely, meaning they were scaled by a factor of $1$. This means that $\\mathbf{a}$ and $\\mathbf{c}$ are eigenvectors of $\\mathbf{B}$, both with eigenvalues of $\\lambda = 1$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
